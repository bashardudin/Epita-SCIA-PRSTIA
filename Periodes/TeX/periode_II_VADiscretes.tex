\documentclass[11pt, a4paper]{article}

\usepackage[french]{babel}
\usepackage{fancyhdr}
\usepackage[margin=.8in]{geometry}

\usepackage{Style/TeXingStyle}
\zexternaldocument*{periode_I_Formalisme.tex}


\pagestyle{fancy}
\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{0.5pt}
\fancyhead[L]{EPITA\_ING2\_2020\_S8}
\fancyhead[R]{Majeure SCIA}
\fancyhead[C]{PRSTIA}
\fancyfoot[C]{\thepage}
\fancyfoot[L]{2019}
\fancyfoot[R]{\textbf{Chargé de cours :} \textsc{Bashar~DUDIN}}

\pretitle{\vspace{-2.5\baselineskip} \begin{center}}
\title{%
  { \huge Variables Aléatoires Discrètes}%
}
\posttitle{
\end{center}
\rule{\textwidth}{1.5pt}
\vspace{-3\baselineskip}
}
\author{}
\date{}

\pdfinfo{
   /Author (Bashar Dudin)
   /Title  (VA Discrètes - 2019)
   /Subject (SCIA - Probabilités)
}

\setlength\parindent{0pt}

\begin{document}

\maketitle\thispagestyle{fancy}

\begin{abstract}
  Cette période (relativement courte) est consacrée à l'étude du cas
  particulier des variables aléatoires discrètes. C'est un cadre
  propice à la mise en pratique des concepts abordés jusqu'à présent,
  sans la difficulté technique qui vient avec la manipulation des
  variables aléatoires à densité.
\end{abstract}

\tableofcontents

\section{Germe d'une VA discrète}
\label{sec:germeVAdiscrete}

Dans l'ensemble de cette feuille $(\Omega, \mc{A}, \PP)$ désigne un
espace probabilisé muni de sa tribu $\mc{A}$ et d'une probabilité
$\PP$.
\begin{defn}
  Une \emph{variable aléatoire discrète} sur $(\Omega, \mc{A}, \PP)$
  est une variable aléatoire à valeurs dans un espace probabilisable
  $\big(\Pi, \mc{P}(\Pi)\big)$ où $\Pi$ est un ensemble
  \textit{dénombrable}.
\end{defn}
\begin{rem}
  Dans la majorité des cas les VAs d'intérêt sont à valeurs dans un
  sous-ensemble de $\R$, on limitera les définitions dans la suite à
  ce cas. On suppose donc qu'une VA discrète est à valeurs dans $\R$.
  Les valeurs que prend la VA sont dans ce cas indexées par les
  entiers naturelles, elles forment un espace probabilisable muni de
  la tribu de l'ensemble de ses parties.
\end{rem}
On appelle \emph{germe} d'une VA aléatoire discrète
$X : \Omega \to \R$ la fonction $g$ donnée par la loi de $X$ sur les
issues. Plus formellement,
\[
  \forall x \in \R, \qquad g(x) = \PP(X \in \{x\})
\]
On parle dans ce contexte du \emph{germe de $\bs{X}$ en $x$}.
\begin{nota}
  On simplifie la notation précédente en notant $\PP(X = x)$ la
  probabilité $\PP(X \in \{x\})$.
\end{nota}
\begin{question}
  En utilisant les axiomes définissant une probabilité montrer que la
  loi d'une VA discrète $X : \Omega \to \R$ est caractérisée par ses
  germes en tout point.
\end{question}

\section{Indépendance et conditionnement}
\label{sec:independanceetconditionnement}

Les notions d'indépendance et de conditionnement de VAs se décrivent
relativement simplement dans le cas des VAs discrètes.
\begin{question}
  Justifier le fait que deux VAs discrètes $X_1$ et $X_2$ sur
  $(\Omega, \mc{A}, \PP)$ sont indépendantes si et seulement si pour
  tout couple d'issues $(x_1, x_2) \in X_1(\Omega)\times X_2(\Omega)$
  on a
  \[
    \PP(X_1 = x_1, X_2 = x_2) = \PP(X_1 = x_1)\PP(X_2 = x_2).
  \]
\end{question}
On reprend les notations ci-dessus. On définit la \emph{loi de
  $\bs{X_2}$ conditionnelle à l'évènement $\bs{(X_1 = x_1)}$} par le
germe
\[
  \PP_{X_2}^{(X_1 = x_1)} := \PP^{(X_1 = x_1)}(X_2 = x_2) = \PP(X_2 =
  x_2 | X_1 = x_1).
\]
\begin{question}
  Montrer que la loi du couple $(X_1, X_2)$ est déterminée par
  $\PP_{X_1}$ et $\PP_{X_2}^{(X_1 = x_1)}$.
\end{question}

\section{Comparer deux VAs}
\label{sec:comparerdeuxVAs}

Il arrive souvent qu'on ait à étudier la réalisation simultanée des
issues de deux VAs $X_1 : \Omega_1 \to \R$ et $X_2 : \Omega_2 \to \R$
sur les espaces probabilisés respectivement donnés par
$(\Omega_1, \mc{A}, \PP_1)$ et $(\Omega_2, \mc{A}, \PP_2)$.  Dans la
démarche menée jusqu'à présent il nous fallait définir un espace
probabilisable sur lequel on peut définir à la fois $X_1$ et $X_2$. En
réalité, rendre explicite un tel espace n'est pas d'une grande
importance dans la pratique ; il suffit de connaître les probabilités
des issues simultanées et cela est uniquement déterminé par
l'expérience aléatoire. La section qui suit a pour objectif de
clarifier en quoi cette phrase fait sens.

Il est facile d'exhiber un espace d'états sur lesquelles les VAs $X_1$
et $X_2$ vivent naturellement : on considère tout simplement le
produit cartésien $\Omega = \Omega_1\times \Omega_2$. On est tenté de
prendre pour tribu sur $\Omega$ l'ensemble des évènements $A \times B$
où $A \in \mc{A}$ et $B \in \mc{B}$.
\begin{question}
  Montrer en quoi la définition ci-dessus ne définit pas une tribu sur
  $\Omega$.
\end{question}
Pour résoudre ce problème on considère la plus petite tribu contenant
les \textit{carré} d'évènements précédents : la tribu engendrées par
ces carrés\footnote{Penser aux sous-espaces vectoriels engendrés par
  une partie.}. Cette tribu peut-être construite en considérant les
complémentaires et union dénombrables des \textit{carrés} puis de même
avec les objets obtenus à cette étape et ainsi de suite\footnote{Pour
  une discussion éclairante sur ce point voir
  \href{https://fr.wikipedia.org/wiki/Tribu_engendrée}{\texttt{Wikipedia
      -- Tribu engendrée}}.}.  La magie de cette construction réside
dans le fait qu'il suffit, pour définir une probabilité sur la tribu
engendrée, de la définir sur les éléments qui l'engendre\footnote{Tout
  comme le fait que pour définir une application linéaire il suffit de
  connaître les valeurs qu'elle prend sur une base.}. La vérification
de ce fait est technique et ne fait pas partie du scope de ce cours,
la conclusion seule nous importe ici. On peut désormais retrouver les
deux VAs $X_1$ et $X_2$ sur $\Omega$ par composition à gauche avec les
projections respectivement sur la première et seconde coordonnée.

Il s'agit maintenant de définir une probabilité sur $\Omega$. On
suppose pour cette période que les VAs $X_1$ et $X_2$ sont
discrètes. D'après la discussion précédente il suffit de définir
celle-ci sur les \textit{carré}, les axiomes d'une probabilité
permettent d'étendre cette définition à tout type d'évènements du
produit. Dans notre cas, on est uniquement concernés par la sous-tribu
engendrée par les carrés du type $(X_1 = x_1) \times (X_2 \times x_2)$
pour des éléments $(x_1, x_2) \in \R^2$.
\begin{question}
  Supposons qu'on définisse la probabilité $\PP$ sur $\Omega$ par
  \[
    \forall (x_1, x_2) \in \R^2, \qquad \PP(X_1= x_1, X_2 = x_2) =
    \PP_1(X_1 = x_1)\PP(X_2 = x_2).
  \]
  À quoi correspond un tel choix?
\end{question}
La description précédente ne prend pas en compte le possible
conditionnement de $X_2$ par rapport à $X_1$, on peut cependant
modéliser une telle situation, et c'est rassurant. Supposons donné
pour tout $x_1 \in \R$ un germe de probabilité $\PP_2^{x_1}$
correspondant à la réalisation d'une issue de $X_2$ conditionnée à la
réalisation de $x_1$ pour $X_1$.
\begin{prop}
  L'expression définie pour tout $(x_1, x_2) \in \R^2$, par
  $\PP(X_1 = x_1, X_2 = x_2) = \PP_1(X_1 = x_1)\PP_2^{x_1}(X_2 = x_2)$
  définit une probabilité sur $\Omega$ qui conditionne $X_2$ par
  $X_1$.
\end{prop}
Cette propriété nous apporte les garanties nécessaires pour comparer
les réalisations simultanées de deux variables ; la généralisation au
cas d'un nombre fini de VAs n'est qu'une difficulté technique. On
renvoie à \cite[pages 65-67 \& 93]{ouvrard1998probabilites}.

\section{VAs discrètes usuelles}
\label{sec:VAsdiscretesusuelles}

On liste dans la suite les VAs usuelles, d'utilisations constantes
dans les cas d'études pratiques et de modélisation.

\subsection{Loi uniforme}
\label{sec:loiuniforme}

Une variable aléatoire $X : \Omega \to \R$ ayant une image finie
$\{x_1, \ldots, x_n\}$ suit une \emph{loi uniforme} si
\[
  \forall i \in \{1, \ldots, n\}, \qquad \PP(X = x_i) = \frac{1}{n}.
\]
On écrit dans ce cas que $\PP_X \leadsto \mc{U}(n)$.
\begin{question}
  Vérifier que cette description correspond à la notion de
  l'équiprobabilité de réalisation de chacune des issues de $X$.
\end{question}

\subsection{Loi de \textsc{Bernoulli}}
\label{sec:loibernoulli}

Une variable aléatoire $X : \Omega \to \R$ d'image
$\{0 , 1\}$ est dite \emph{loi de \textsc{Bernoulli} de paramètre
  $\bs{p}$} si
\[
  \PP(X = 1) = p.
\]
\begin{question}
  En quoi est-ce que la donnée de la valeur de $\PP_X$ sur $1$ est
  suffisante pour décrire $\PP_X$?
\end{question}

\subsubsection{\emph{Score} d'un classificateur}
\label{sec:classificateurAlea}

On chercher dans cette section à quantifier la \textit{qualité} d'un
classificateur binaire. Les classifcateurs sont des modèles de ML
utilisés lorsque l'on souhaite prédire une caractéristique discrète ;
par exemple des types de plantes, des couleurs de cheveux, des
appréciations de goûts etc. à partir d'observations en entrée. Un
classificateur binaire n'a que deux valeurs de sorties qui correspond
à l'existence ou non d'une caractéristique que l'on cherche à observer
; avoir une maladie ou non, réussir un examen ou non, etc.

Dans la formalisation qu'on propose ici on considère que les
observations en entrée constituent un espace d'états $\Omega$. On
considère de plus que celui-ci vient avec une fonction $\lambda$ qu'on
désigne par \textit{label} qui code la caractéristique qu'on cherche à
prédire. Autrement dit on suppose travailler avec un dataset en entrée
qui contient cette information ; on est dans un cadre d'apprentissage
supervisé. Dans ce contexte un classificateur est une variable
aléatoire $X : \Omega \to F$ où $F$ est l'espace des labels qu'on
cherche à attribuer à nos entrées, c'est-à-dire $F = \{0, 1\}
$. L'espace $\Omega$ est supposé fini probabilisé muni d'une
probabilité $\PP$. C'est cette probabilité qui fait office de
\textit{proportion}. On suppose que $X$ est donnée et on souhaite
étudier trois quantités qui sont liées à l'évaluation de la qualité
d'un classificateur.
\begin{question}
  On appelle \emph{précision totale} d'un classificateur binaire $X$
  la proportion des individus bien classés, c'est-à-dire ceux où les
  réponses de $X$ et $\lambda$ coïncident.
  \begin{enumerate}
  \item Exprimer la précision totale de $X$ en s'aidant de la variable
    aléatoire $X - \lambda$.
  \item Déterminer la loi de $X - \lambda$ et interpréter ses deux
    autres valeurs.
  \end{enumerate}
\end{question}
Dans le cas d'un classificateur binaire deux autres quantités
apparaissent relativement naturellement à l'évaluation :
\begin{itemize}
\item la \emph{précision}: proportion des vrais positifs par
  rapport à l'ensemble des bonnes classifications.
\item le \emph{rappel}: proportion des vrais positifs par rapport
  à l'ensemble des positifs (donnés par $\lambda$).
\end{itemize}
\begin{question}
  \begin{enumerate}
  \item Exprimer précision et rappel de $X$.
  \item Étudier l'interconnexion entre précision et rappel ;
    qu'arrive-t-il à l'une si l'autre augmente?
  \end{enumerate}
\end{question}
Dans les problématiques de classification on est toujours attentifs au
fait d'être confrontés à des modèles dont la dépendance au label
$\lambda$ est très faible. C'est pour cette raison qu'on va souvent
chercher à évaluer le \emph{score} d'un classificateur $X$ tels que
$X$ et $\lambda$ définissent des variables aléatoires indépendantes.
\begin{question}
  \begin{enumerate}
  \item Simplifier les expressions de la précision totale, précision
    et rappel dans le cas où $X$ et $\lambda$ sont indépendants.
  \item On suppose que $X$ et $\lambda$ sont indépendants et suivent
    des loi de \textsc{Bernoulli} respectivement de paramètres $p$ et
    $q$. Exprimer les différents scores de $X$ dans ce cas.
  \item Quels résultats numériques obtenez vous pour chacun des scores
    quand $p=q=0.9$? Qu'en déduisez vous?
  \end{enumerate}
\end{question}

\subsection{Loi géométrique}
\label{sec:loigéométrique}

On considère l'expérience aléatoire qui consiste à jeter une pièce
truquée autant de fois qu'on veut. On suppose qu'on a $p \in ]0, 1[$
chances d'avoir Pile. On note $X : \Omega \to \R$ la variable
aléatoire qui consiste à obtenir le premier Face.
\begin{question}
  Décrire la loi de la VA $X$. Quelle hypothèse implicite on utilise
  dans cette description?
\end{question}
La VA $X$ est dite suivre une \emph{loi géométrique de paramètre
  $\bs{p}$}.
\begin{question}
  Reprenez la question \zref{q:brendan} et supposez que Brendan remets
  à chaque essai le briquet testé dans sa poche. Décrire la loi de la
  VA Brendan a réussi à allumer sa cigarette.
\end{question}

\subsection{Loi binômiale}
\label{sec:loibinomiale}

Une variable aléatoire $X : \Omega \to \R$ d'image dans
$\{0, \ldots, n\}$ suit une loi binômiale de paramètres $n$ et $p$,
qu'on écrit $\PP_X \leadsto \mc{B}(n, p)$, si elle est de la forme
\[
  X = \sum_{i = 1}^n B_i
\]
pour des variables de \textsc{Bernoulli} $B_i$ de paramètre $p$, indépendantes
dans leur ensembles.
\begin{question}
  Expliciter une formule pour la loi de $X$ suivant une
  $\mc{B}(n, p)$.
\end{question}
\begin{question}
  Décrire des expériences aléatoires modélisées par la loi précédente.
\end{question}

\subsection{Loi hypergéométrique}
\label{sec:loihypergeometrique}

On considère l'expérience aléatoire qui consiste à tirer au hasard $k$
boules dans une urnes contenant $n_b$ boules blanches et $n_r$ boules
rouges.
\begin{question}
  Décrire la loi de la VA $X$ qui compte le nombre de boules rouges.
\end{question}
\begin{question}
  Revenez à la seconde partie de la question \zref{q:repartitionTD}.
\end{question}

\subsection{Loi de \textsc{Poisson}}
\label{sec:loidepoisson}

Vous pouvez retrouver la description suivante dans
\cite{aleaprobasIX}. On considère une suite $(p_n)$ avec
$np_n \to \lambda \in \R_+^*$. On s'intéresse à la suite de VAs
$X_n \leadsto \mc{B}(n, p_n)$. C'est une suite de VAs suivant des lois
binômiales dont le paramètre de réalisation est de plus en petit. Ce
processus est une tentative de modélisation d'évènements rares. Pour
tout $ k \in \N$
\[
  \PP(X_n = k) = \left\{
    \begin{matrix}
      \binom{k}{n}p_n^k(1-p_n)^{n-k} & \textrm{si $k \leq n$} \\
      0 & \textrm{si $k \geq n+1$}
    \end{matrix}\right.
\]
\begin{question}
  \begin{enumerate}
  \item Montrer que
    $\lim_{n \to +\infty} \PP(X_n = k) =
    e^{-\lambda}\frac{\lambda^k}{k!}$.
  \item Vérifier que la donnée pour tout $k \in \N$, par
    $g(k) = e^{-\lambda}\frac{\lambda^k}{k!}$ définit une loi de
    probabilité sur $\N$.
  \end{enumerate}
\end{question}
Une VA $X$ qui suit la loi décrite précédemment est dite de
\textsc{Poisson} de paramètre $\lambda$, on écrite
$X \leadsto \mc{P}(\lambda)$.
\begin{question}
  Vous retrouverez cet exemple dans \cite[page
  94]{ouvrard1998probabilites}. On se place à un embranchement routier
  ; le nombre de véhicules arrivant pendant un intervalle d'une heure
  est une VA $X$, suivant une loi de \textsc{Poisson} de paramètre
  $\lambda$. Les véhicules ne peuvent prendre qu'une des deux
  directions $A$ ou $B$, et la VA $Y$ représente le nombre de
  véhicules empruntant la direction $A$ pendant cet intervalle de
  temps. Chaque véhicule prend la direction $A$ avec probabilité $p$,
  et les choix sont faits de manière indépendante. C'est pour cette
  raison qu'on suppose que si $n$ véhicules arrivent à l'embranchement
  pendant une heure donnée, la loi (conditionnelle) de
  $Y \leadsto \mc{B}(n, p)$..

  La probabilité conditionnelle $\PP^{(Y = k)}(X = n)$ est la
  probabilité que $n$ véhicules soient arrivés à l'embranchement
  sachant que $k$ véhicules ont emprunté la direction $A$. Calculer
  cette probabilité.
\end{question}

\section{Moments d'une VA discrète}
\label{sec:momentsVAdiscrète}

L'étude du comportement aléatoire d'une VA discrète s'étudie souvent
par le biais de mesures de \emph{centrage} et de
\emph{dispersion}. Dans le jargon quotidien on parlera par exemple de
\textit{moyenne} ou \textit{médiane} et de \textit{variance} ou
\textit{écart-type}. Ces deux mesures, qu'on pense souvent en termes
statistiques se calcule par le biais des \emph{moments} d'ordre $1$ et
$2$ des VAs qui modélisent les phénomènes à l'étude.

\subsection{Espérance d'une VA discrète}
\label{sec:esperance}

L'équivalent de la notion de \emph{moyenne} en théorie des
probabilités est celui d'espérance.
\begin{defn}[Espérance d'une VA discrète]
  Soit $X : \Omega \to \R$ une VA discrète. Si
  $\sum_{x\in \R} |x|\PP(X = x) < + \infty$ on appelle
  \emph{espérance} de la VA $X$ la quantité
  \[
    \EE(X) = \sum_{x \in X} x\PP(X = x).
  \]
\end{defn}
\begin{question}
  Calculer l'espérance d'une VA discrète suivant une loi uniforme sur
  $\{0, \ldots, n\}$.
\end{question}
\begin{question}
  Quelle est l'espérance d'une loi de \textsc{Bernoulli} de paramètre $p$?
\end{question}
\begin{question}
  \begin{enumerate}
  \item
    Soit $X$, $Y$ deux VA discrètes sur $(\Omega, \mc{A}, \PP)$. Étant
    donnée $\lambda \in \R$, montrer que
    \begin{equation}
      \label{eq:linearitesperance}
      \tag{Linéarité de l'espérance}
      \EE(\lambda X + Y) = \lambda\EE(X) + \EE(Y).
    \end{equation}
  \item En déduire l'espérance d'une VA suivant une loi binômiale de
    paramètres $n$ et $p$.
  \end{enumerate}
\end{question}
\begin{question}
  Vérifier les propriétés suivantes de l'espérance d'une VA
  $X : \Omega \to \R$.
  \begin{enumerate}
  \item Si $c$ est l'unique valeur que prend $X$ alors $\EE(X) = c$.
  \item Si $X$ est bornée $X$ admet une espérance.
  \item Si $X \geq 0$ alors $\EE(X) \geq 0$.
  \item $|\EE(X)| \leq \EE(|X|)$.
  \end{enumerate}
\end{question}
Il arrive souvent qu'on s'intéresse à une fonction d'une VA
$X : \Omega \to \Pi$, c'est-à-dire à la fonction composée
$f(X) = f\circ X$ pour $f : \Pi \to \Delta$. Le théorème de transfert
nous indique qu'on peut étudier la loi de $f(X)$ en travaillant
toujours sur $\Pi$. On suppose que $X$ est une VA discrète ; il en va
de même de $f(X)$.
\begin{thm}[Théorème de transfert]
  Si la somme $\sum_{x \in \Pi}|f(x)|\PP(X = x)$ est finie alors
  \[
    \EE\big(f(X)\big) = \sum_{x \in \Pi}f(x)\PP(X = x).
  \]
\end{thm}
\begin{question}
  Calculer les espérances des lois usuelles restantes dans la liste en
  amont.
\end{question}

\subsection{Variance d'une VA discrète}
\label{sec:varianceVAdiscrete}

La variance est une mesure de dispersion, autrement dit de variabilité
autour de l'espérance d'une VA discrète.
\begin{defn}
  Soit $X : \Omega \to \R$ une VA discrète. Si $\EE(X^2) < + \infty$
  on appelle \emph{variance} de la VA $X$ la quantité
  \[
    \VV(X) = \EE\big( (X- \EE(X))^2\big)
  \]
\end{defn}
On appelle \emph{écrat-type} d'une VA $X$ admettant une variance le
scalaire
\[
  \sigma_X = \sqrt{\VV(X)}
\]
\begin{question}
  Soit $X$ une VA discrète qui admet une variance. Montrer que
  \[
    \VV(X) = \EE(X^2) - \EE(X)^2.
  \]
\end{question}
\begin{question}
  Quel est l'équivalent de la relation (\ref{eq:linearitesperance})
  dans le cas de la variance d'une loi aléatoire discrète $X$ sur
  $(\Omega, \mc{A}, \PP)$.
\end{question}
\begin{question}
  Calculer les variances des VAs discrètes suivant les lois usuelles
  listées en amont. Identifier parmi celles-ci celles qui vous posent
  problème.
\end{question}
Centrer et réduire ses données est un \textit{pre-processing} standard
pour différents types d'algorithmes de ML et pour différentes
raisons. L'objectif peut par exemple être d'optimiser une descente de
gradient ou d'étudier la corrélation de \textit{features} différentes.
\begin{defn}
  La VA \emph{centrée réduite} associée à une VA $X : \Omega \to \R$
  admettant une espérance et une variance est la VA
  \[
    \overline{X} = \frac{X - \EE(X)}{\sigma_X}.
  \]
\end{defn}
\begin{question}
  Vérifier que $\EE(\overline{X}) = 0$ et $\VV(\overline{X}) = 1$
\end{question}

\subsection{Moments d'ordres supérieures d'une VA discrète}
\label{sec:momentsups}

L'espérance et la variance sont des mesures correspondant au moment
d'ordre $1$ dans le premier cas et à partir de ceux d'ordres $1$ et
$2$ dans le second.
\begin{defn}
  Soient $X : \Omega \to \R$ une VA discrète et $p \geq 1$. Si
  $\EE(|X|^p) < +\infty$ on appelle moment d'ordre $p$ de $X$ la
  quantité
  \[
    \mu_p(X) = \EE(X^p).
  \]
\end{defn}
Avec les notations précédentes on peut écrire la variance de $X$ sous
la forme
\[
  \VV(X) = \mu_2(X) - \mu_1(X)^2.
\]
La manipulation des moments d'ordre supérieurs de variables discrètes
n'est pas aisée avec les outils qu'on a en main. Il nous faudrait
introduire dans ce but les séries génératrices de VAs
discrètes. Notion qui, malgré son intérêt\footnote{Et son
  esthétisme!}, nous pousserait loin du scope de ce cours. Il est
cependant important d'en connaître l'existence et la définition ; ces
moments sont utilisés dans les techniques de représentations de
données.

\section{Covariance et corrélation}
\label{sec:convarianceetcorrelation}


\bibliographystyle{alpha}
\bibliography{PRSTIA}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
